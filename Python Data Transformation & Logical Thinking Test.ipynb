{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3755b26",
   "metadata": {},
   "source": [
    "1.\n",
    "(15 Pts.) Construct an array of 1 million numbers in the range between (-10000.00 to 10000.00).\n",
    "•\n",
    "Write a code that gives the prime numbers in form of a list without using any library that directly gives the prime numbers as an output.\n",
    "•\n",
    "Write a code that sums up the square of all the negative numbers and cubes of all the positive numbers. Can this value ever be negative?\n",
    "•\n",
    "Write an algorithm that sorts the number in an arrangement where the first number is the largest, second number is Smallest, third number is second largest, fourth number is second smallest so on.\n",
    "i.\n",
    "For example, if input is: [-5, -3.2, 0, 1, 3, -6,10]\n",
    "ii.\n",
    "then the output should be: [10, -6, 3, -5, 1 -3.2, 0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "383a2955",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# array construction  \n",
    "array = np.random.uniform(-10000.00, 10000.00, 1000000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0e52d476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[443, 7577, 4099, 8803, 3011, 5309, 337, 5351, 9311, 47]\n"
     ]
    }
   ],
   "source": [
    "def is_prime(num):\n",
    "    if num < 2:\n",
    "        return False\n",
    "    for i in range(2, int(abs(num)**0.5) + 1):\n",
    "        if num % i == 0:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "# prime numbers Filter \n",
    "prime_numbers = [int(num) for num in array if is_prime(int(num))]\n",
    "\n",
    "# Display of first 10 numbers found \n",
    "print(prime_numbers[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8ac75bdb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sum of squares of negatives and cubes of positives: 1.2503237089898534e+17\n"
     ]
    }
   ],
   "source": [
    "# Sum squares of negative numbers and cubes of positive numbers\n",
    "sum_squares_neg = np.sum([num**2 for num in array if num < 0])\n",
    "sum_cubes_pos = np.sum([num**3 for num in array if num > 0])\n",
    "\n",
    "result_sum = sum_squares_neg + sum_cubes_pos\n",
    "\n",
    "#result\n",
    "print(f\"Sum of squares of negatives and cubes of positives: {result_sum}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "1e7952a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sorted array (first 10 elements): [9999.943769435831, -9999.95873479853, 9999.943337811023, -9999.950809588165, 9999.941168299796, -9999.948695162515, 9999.94077643859, -9999.942763183843, 9999.921162613682, -9999.898827449631]\n",
      "Execution time: 1.4552934169769287 seconds\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "\n",
    "def custom_sort(arr):\n",
    "    arr_sorted = sorted(arr)\n",
    "    result = []\n",
    "    i, j = 0, len(arr_sorted) - 1\n",
    "    while i <= j:\n",
    "        if i != j:\n",
    "            result.append(arr_sorted[j])\n",
    "            result.append(arr_sorted[i])\n",
    "        else:\n",
    "            result.append(arr_sorted[j])\n",
    "        i += 1\n",
    "        j -= 1\n",
    "    return result\n",
    "\n",
    "# Time the sorting process\n",
    "start_time = time.time()\n",
    "\n",
    "# Sort the array\n",
    "sorted_array = custom_sort(array)\n",
    "\n",
    "execution_time = time.time() - start_time\n",
    "\n",
    "# first 10 elements of the sorted array and the execution time\n",
    "print(f\"Sorted array (first 10 elements): {sorted_array[:10]}\")\n",
    "print(f\"Execution time: {execution_time} seconds\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f5495f0",
   "metadata": {},
   "source": [
    "Load the following three Yelp dataset (mentioned below). You can read more about the Yelp review data here. These datasets are relatively large so avoid directly loading it into your memory/data frame. The objective of this task is to assess your ability to handle large data efficiently even under resource constraint.\n",
    "i.\n",
    "yelp_academic_dataset_review\n",
    "ii.\n",
    "yelp_academic_dataset_business\n",
    "iii.\n",
    "yelp_academic_dataset_user\n",
    "•\n",
    "Perform the following task once all your 3 datasets are in place:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ee26fe1",
   "metadata": {},
   "source": [
    "Loading the Yelp Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "e844bf46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6023.610883712769\n",
      "6177.040030479431\n",
      "9474.322436332703\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "chunk_memory = []\n",
    "def load_and_analyze (file_name):\n",
    "    for chunk in pd.read_json(file_name,lines=True, chunksize=10000):\n",
    "#         print(pd.read_json(file_name).column.dtype)\n",
    "        chunk_memory.append(chunk.memory_usage(deep=True).sum())\n",
    "        \n",
    "    total_memory = sum(chunk_memory)\n",
    "    print(total_memory/1024**2)\n",
    "        \n",
    "review_chunks = load_and_analyze('yelp_academic_dataset_review.json')\n",
    "business_chunks = load_and_analyze('yelp_academic_dataset_business.json')\n",
    "user_chunks = load_and_analyze('yelp_academic_dataset_user.json')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "1dcfda34",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "read_json() got an unexpected keyword argument 'line'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[44], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# def optimize_data_types(path):\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m data \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp_academic_dataset_review.json\u001b[39m\u001b[38;5;124m'\u001b[39m,line\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m df:\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#     select_dtype(include =['object']).columns:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(col)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "\u001b[1;31mTypeError\u001b[0m: read_json() got an unexpected keyword argument 'line'"
     ]
    }
   ],
   "source": [
    "# def optimize_data_types(path):\n",
    "import pandas as pd\n",
    "data = pd.read_json('yelp_academic_dataset_review.json',line=True)\n",
    "for col in df:\n",
    "#     select_dtype(include =['object']).columns:\n",
    "    print(col)\n",
    "print(df.dtypes)\n",
    "#         df[col] = df[col].astype('category')\n",
    "        \n",
    "# review_chunks = optimize_data_types('yelp_academic_dataset_review.json')\n",
    "# business_chunks = optimize_data_types(pd.read_json('yelp_academic_dataset_business.json'))\n",
    "# user_chunks = optimize_data_types(pd.read_json('yelp_academic_dataset_user.json'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f50ebb6a",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Index contains duplicate entries, cannot reshape",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[50], line 54\u001b[0m\n\u001b[0;32m     51\u001b[0m output_path_1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mProcessed_Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m     52\u001b[0m output_path_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mReshaped_Data.csv\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 54\u001b[0m process_tax_audit_data(sample_data_path, master_data_path, output_path_1, output_path_2)\n",
      "Cell \u001b[1;32mIn[50], line 41\u001b[0m, in \u001b[0;36mprocess_tax_audit_data\u001b[1;34m(sample_data_path, master_data_path, output_path_1, output_path_2)\u001b[0m\n\u001b[0;32m     39\u001b[0m \u001b[38;5;66;03m# Reshape data for Output 2\u001b[39;00m\n\u001b[0;32m     40\u001b[0m fixed_particular \u001b[38;5;241m=\u001b[39m sample_data[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticulars\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39miloc[\u001b[38;5;241m0\u001b[39m]\n\u001b[1;32m---> 41\u001b[0m reshaped_data \u001b[38;5;241m=\u001b[39m sample_data\u001b[38;5;241m.\u001b[39mpivot(index\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDate\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVch Type\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVch No.\u001b[39m\u001b[38;5;124m'\u001b[39m], columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mParticulars\u001b[39m\u001b[38;5;124m'\u001b[39m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAmount\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;241m.\u001b[39mreset_index()\n\u001b[0;32m     42\u001b[0m reshaped_data\u001b[38;5;241m.\u001b[39minsert(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFixed Particular\u001b[39m\u001b[38;5;124m'\u001b[39m, fixed_particular)\n\u001b[0;32m     44\u001b[0m \u001b[38;5;66;03m# Save the second output (Reshaped Data)\u001b[39;00m\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\frame.py:8567\u001b[0m, in \u001b[0;36mDataFrame.pivot\u001b[1;34m(self, index, columns, values)\u001b[0m\n\u001b[0;32m   8561\u001b[0m \u001b[38;5;129m@Substitution\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m   8562\u001b[0m \u001b[38;5;129m@Appender\u001b[39m(_shared_docs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpivot\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8563\u001b[0m \u001b[38;5;129m@deprecate_nonkeyword_arguments\u001b[39m(version\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, allowed_args\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mself\u001b[39m\u001b[38;5;124m\"\u001b[39m])\n\u001b[0;32m   8564\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpivot\u001b[39m(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, values\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame:\n\u001b[0;32m   8565\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpivot\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pivot\n\u001b[1;32m-> 8567\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m pivot(\u001b[38;5;28mself\u001b[39m, index\u001b[38;5;241m=\u001b[39mindex, columns\u001b[38;5;241m=\u001b[39mcolumns, values\u001b[38;5;241m=\u001b[39mvalues)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\reshape\\pivot.py:540\u001b[0m, in \u001b[0;36mpivot\u001b[1;34m(data, index, columns, values)\u001b[0m\n\u001b[0;32m    536\u001b[0m         indexed \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39m_constructor_sliced(data[values]\u001b[38;5;241m.\u001b[39m_values, index\u001b[38;5;241m=\u001b[39mmultiindex)\n\u001b[0;32m    537\u001b[0m \u001b[38;5;66;03m# error: Argument 1 to \"unstack\" of \"DataFrame\" has incompatible type \"Union\u001b[39;00m\n\u001b[0;32m    538\u001b[0m \u001b[38;5;66;03m# [List[Any], ExtensionArray, ndarray[Any, Any], Index, Series]\"; expected\u001b[39;00m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;66;03m# \"Hashable\"\u001b[39;00m\n\u001b[1;32m--> 540\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m indexed\u001b[38;5;241m.\u001b[39munstack(columns_listlike)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\series.py:4455\u001b[0m, in \u001b[0;36mSeries.unstack\u001b[1;34m(self, level, fill_value)\u001b[0m\n\u001b[0;32m   4412\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4413\u001b[0m \u001b[38;5;124;03mUnstack, also known as pivot, Series with MultiIndex to produce DataFrame.\u001b[39;00m\n\u001b[0;32m   4414\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4451\u001b[0m \u001b[38;5;124;03mb    2    4\u001b[39;00m\n\u001b[0;32m   4452\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4453\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcore\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mreshape\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m unstack\n\u001b[1;32m-> 4455\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstack(\u001b[38;5;28mself\u001b[39m, level, fill_value)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:489\u001b[0m, in \u001b[0;36munstack\u001b[1;34m(obj, level, fill_value)\u001b[0m\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_1d_only_ea_dtype(obj\u001b[38;5;241m.\u001b[39mdtype):\n\u001b[0;32m    488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unstack_extension_series(obj, level, fill_value)\n\u001b[1;32m--> 489\u001b[0m unstacker \u001b[38;5;241m=\u001b[39m _Unstacker(\n\u001b[0;32m    490\u001b[0m     obj\u001b[38;5;241m.\u001b[39mindex, level\u001b[38;5;241m=\u001b[39mlevel, constructor\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39m_constructor_expanddim\n\u001b[0;32m    491\u001b[0m )\n\u001b[0;32m    492\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m unstacker\u001b[38;5;241m.\u001b[39mget_result(\n\u001b[0;32m    493\u001b[0m     obj\u001b[38;5;241m.\u001b[39m_values, value_columns\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, fill_value\u001b[38;5;241m=\u001b[39mfill_value\n\u001b[0;32m    494\u001b[0m )\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:137\u001b[0m, in \u001b[0;36m_Unstacker.__init__\u001b[1;34m(self, index, level, constructor)\u001b[0m\n\u001b[0;32m    129\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m num_cells \u001b[38;5;241m>\u001b[39m np\u001b[38;5;241m.\u001b[39miinfo(np\u001b[38;5;241m.\u001b[39mint32)\u001b[38;5;241m.\u001b[39mmax:\n\u001b[0;32m    130\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    131\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe following operation may generate \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_cells\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m cells \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    132\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124min the resulting pandas object.\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    133\u001b[0m         PerformanceWarning,\n\u001b[0;32m    134\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    135\u001b[0m     )\n\u001b[1;32m--> 137\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_selectors()\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\core\\reshape\\reshape.py:189\u001b[0m, in \u001b[0;36m_Unstacker._make_selectors\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    186\u001b[0m mask\u001b[38;5;241m.\u001b[39mput(selector, \u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m    188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m mask\u001b[38;5;241m.\u001b[39msum() \u001b[38;5;241m<\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mindex):\n\u001b[1;32m--> 189\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex contains duplicate entries, cannot reshape\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    191\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroup_index \u001b[38;5;241m=\u001b[39m comp_index\n\u001b[0;32m    192\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmask \u001b[38;5;241m=\u001b[39m mask\n",
      "\u001b[1;31mValueError\u001b[0m: Index contains duplicate entries, cannot reshape"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
    "\n",
    "def process_tax_audit_data(sample_data_path, master_data_path, output_path_1, output_path_2):\n",
    "    \n",
    "    sample_data = load_data(sample_data_path)\n",
    "    master_data = load_data(master_data_path)\n",
    "    \n",
    "    # Propagate Date, Vch Type, Vch No down the group\n",
    "    sample_data['Date'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Date'].transform('first')\n",
    "    sample_data['Vch Type'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Vch Type'].transform('first')\n",
    "    sample_data['Vch No.'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Vch No.'].transform('first')\n",
    "    \n",
    "    sample_data['Particulars'] = sample_data['Particulars'].astype(str)\n",
    "    \n",
    "    # narration rows and bring narration to a new column\n",
    "    sample_data['Narration'] = np.where(sample_data['Particulars'].str.contains('Narration',na=False), sample_data['Particulars'], np.nan)\n",
    "    sample_data['Narration'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Narration'].ffill().bfill()\n",
    "    sample_data = sample_data[~sample_data['Particulars'].str.contains('Narration', na=False)]\n",
    "    \n",
    "    # Debit and Credit amounts\n",
    "    sample_data['Debit Amount'] = sample_data['Debit Amount'].abs()\n",
    "    sample_data['Credit Amount'] = -sample_data['Credit Amount'].abs()\n",
    "    \n",
    "    # V-Lookup from Master file\n",
    "    sample_data = pd.merge(sample_data, master_data[['Particulars', 'Header']], on='Particulars', how='left')\n",
    "    \n",
    "    # first output (Processed Data)\n",
    "    sample_data.to_csv(output_path_1, index=False)\n",
    "    \n",
    "    # data for Output 2\n",
    "    fixed_particular = sample_data['Particulars'].iloc[0]\n",
    "    reshaped_data = sample_data.pivot(index=['Date', 'Vch Type', 'Vch No.'], columns='Particulars', values='Amount').reset_index()\n",
    "    reshaped_data.insert(0, 'Fixed Particular', fixed_particular)\n",
    "    \n",
    "    # second output (Reshaped Data)\n",
    "    reshaped_data.to_csv(output_path_2, index=False)\n",
    "    print(f\"Processed data saved to {output_path_1} and reshaped data saved to {output_path_2}\")\n",
    "\n",
    "# Example usage\n",
    "sample_data_path = 'Sample Data.csv'  # Replace with actual file path\n",
    "master_data_path = 'Master.csv'       # Replace with actual file path\n",
    "output_path_1 = 'Processed_Data.csv'\n",
    "output_path_2 = 'Reshaped_Data.csv'\n",
    "\n",
    "process_tax_audit_data(sample_data_path, master_data_path, output_path_1, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c5fa3c0c",
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Load datasets\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m business \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp_academic_dataset_review.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      4\u001b[0m review \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp_academic_dataset_business.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      5\u001b[0m user \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_json(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124myelp_academic_dataset_user.json\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:211\u001b[0m, in \u001b[0;36mdeprecate_kwarg.<locals>._deprecate_kwarg.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    209\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    210\u001b[0m         kwargs[new_arg_name] \u001b[38;5;241m=\u001b[39m new_arg_value\n\u001b[1;32m--> 211\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\util\\_decorators.py:331\u001b[0m, in \u001b[0;36mdeprecate_nonkeyword_arguments.<locals>.decorate.<locals>.wrapper\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    325\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m>\u001b[39m num_allow_args:\n\u001b[0;32m    326\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[0;32m    327\u001b[0m         msg\u001b[38;5;241m.\u001b[39mformat(arguments\u001b[38;5;241m=\u001b[39m_format_argument_list(allow_args)),\n\u001b[0;32m    328\u001b[0m         \u001b[38;5;167;01mFutureWarning\u001b[39;00m,\n\u001b[0;32m    329\u001b[0m         stacklevel\u001b[38;5;241m=\u001b[39mfind_stack_level(),\n\u001b[0;32m    330\u001b[0m     )\n\u001b[1;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\io\\json\\_json.py:733\u001b[0m, in \u001b[0;36mread_json\u001b[1;34m(path_or_buf, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, encoding_errors, lines, chunksize, compression, nrows, storage_options)\u001b[0m\n\u001b[0;32m    730\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_axes \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m orient \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtable\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m    731\u001b[0m     convert_axes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m--> 733\u001b[0m json_reader \u001b[38;5;241m=\u001b[39m JsonReader(\n\u001b[0;32m    734\u001b[0m     path_or_buf,\n\u001b[0;32m    735\u001b[0m     orient\u001b[38;5;241m=\u001b[39morient,\n\u001b[0;32m    736\u001b[0m     typ\u001b[38;5;241m=\u001b[39mtyp,\n\u001b[0;32m    737\u001b[0m     dtype\u001b[38;5;241m=\u001b[39mdtype,\n\u001b[0;32m    738\u001b[0m     convert_axes\u001b[38;5;241m=\u001b[39mconvert_axes,\n\u001b[0;32m    739\u001b[0m     convert_dates\u001b[38;5;241m=\u001b[39mconvert_dates,\n\u001b[0;32m    740\u001b[0m     keep_default_dates\u001b[38;5;241m=\u001b[39mkeep_default_dates,\n\u001b[0;32m    741\u001b[0m     numpy\u001b[38;5;241m=\u001b[39mnumpy,\n\u001b[0;32m    742\u001b[0m     precise_float\u001b[38;5;241m=\u001b[39mprecise_float,\n\u001b[0;32m    743\u001b[0m     date_unit\u001b[38;5;241m=\u001b[39mdate_unit,\n\u001b[0;32m    744\u001b[0m     encoding\u001b[38;5;241m=\u001b[39mencoding,\n\u001b[0;32m    745\u001b[0m     lines\u001b[38;5;241m=\u001b[39mlines,\n\u001b[0;32m    746\u001b[0m     chunksize\u001b[38;5;241m=\u001b[39mchunksize,\n\u001b[0;32m    747\u001b[0m     compression\u001b[38;5;241m=\u001b[39mcompression,\n\u001b[0;32m    748\u001b[0m     nrows\u001b[38;5;241m=\u001b[39mnrows,\n\u001b[0;32m    749\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39mstorage_options,\n\u001b[0;32m    750\u001b[0m     encoding_errors\u001b[38;5;241m=\u001b[39mencoding_errors,\n\u001b[0;32m    751\u001b[0m )\n\u001b[0;32m    753\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize:\n\u001b[0;32m    754\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m json_reader\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\io\\json\\_json.py:819\u001b[0m, in \u001b[0;36mJsonReader.__init__\u001b[1;34m(self, filepath_or_buffer, orient, typ, dtype, convert_axes, convert_dates, keep_default_dates, numpy, precise_float, date_unit, encoding, lines, chunksize, compression, nrows, storage_options, encoding_errors)\u001b[0m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnrows can only be passed if lines=True\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m    818\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_data_from_filepath(filepath_or_buffer)\n\u001b[1;32m--> 819\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_preprocess_data(data)\n",
      "File \u001b[1;32mE:\\Data Science files\\Lib\\site-packages\\pandas\\io\\json\\_json.py:831\u001b[0m, in \u001b[0;36mJsonReader._preprocess_data\u001b[1;34m(self, data)\u001b[0m\n\u001b[0;32m    829\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    830\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 831\u001b[0m         data \u001b[38;5;241m=\u001b[39m data\u001b[38;5;241m.\u001b[39mread()\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(data, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchunksize \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnrows):\n\u001b[0;32m    833\u001b[0m     data \u001b[38;5;241m=\u001b[39m StringIO(data)\n",
      "File \u001b[1;32m<frozen codecs>:322\u001b[0m, in \u001b[0;36mdecode\u001b[1;34m(self, input, final)\u001b[0m\n",
      "\u001b[1;31mMemoryError\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    " \n",
    "business = pd.read_json('yelp_academic_dataset_review.json')\n",
    "review = pd.read_json('yelp_academic_dataset_business.json')\n",
    "user = pd.read_json('yelp_academic_dataset_user.json')\n",
    "\n",
    "def missing_data_info(df):\n",
    "    missing_data = df.isnull().sum()  # Count missing values\n",
    "    total_data = len(df)  # Total rows\n",
    "    missing_percentage = (missing_data / total_data) * 100  # Percentage of missing values\n",
    "    \n",
    "    return pd.DataFrame({\n",
    "        'Missing Values': missing_data,\n",
    "        'Percentage': missing_percentage\n",
    "    })\n",
    "\n",
    "# missing data for each dataset\n",
    "business_missing_data = missing_data_info(business)\n",
    "review_missing_data = missing_data_info(review)\n",
    "user_missing_data = missing_data_info(user)\n",
    "\n",
    "# results\n",
    "print(\"Business Dataset Missing Data:\")\n",
    "print(business_missing_data)\n",
    "\n",
    "print(\"\\nReview Dataset Missing Data:\")\n",
    "print(review_missing_data)\n",
    "\n",
    "print(\"\\nUser Dataset Missing Data:\")\n",
    "print(user_missing_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ae9865e",
   "metadata": {},
   "source": [
    "You have been provided with “Sample Data” and “Master” from a client that needs to be audited for tax. This data is coming from a system in forms for report. The client is requesting you to automate the process. You have been provided with the following requirements from a Chartered Accountant who does not have exposure to Python, SQL, or coding in general:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "6a832788",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed data saved to Processed_Data.csv and reshaped data saved to Reshaped_Data.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def load_data(file_path):\n",
    "    if file_path.endswith('.csv'):\n",
    "        return pd.read_csv(file_path)\n",
    "    elif file_path.endswith(('.xls', '.xlsx')):\n",
    "        return pd.read_excel(file_path)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported file format. Please provide a CSV or Excel file.\")\n",
    "\n",
    "def process_tax_audit_data(sample_data_path, master_data_path, output_path_1, output_path_2):\n",
    " \n",
    "    sample_data = load_data(sample_data_path)\n",
    "    master_data = load_data(master_data_path)\n",
    "    \n",
    "    # Date, Vch Type, Vch No down the group\n",
    "    sample_data['Date'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Date'].transform('first')\n",
    "    sample_data['Vch Type'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Vch Type'].transform('first')\n",
    "    sample_data['Vch No.'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Vch No.'].transform('first')\n",
    "    \n",
    "     sample_data['Particulars'] = sample_data['Particulars'].astype(str)\n",
    "    \n",
    "    # rows and bring narration to a new column\n",
    "    sample_data['Narration'] = np.where(sample_data['Particulars'].str.contains('Narration', na=False), sample_data['Particulars'], np.nan)\n",
    "    sample_data['Narration'] = sample_data.groupby(['Vch Type', 'Vch No.'])['Narration'].ffill().bfill()\n",
    "    \n",
    "    # rows that are identified as narration\n",
    "    sample_data = sample_data[~sample_data['Particulars'].str.contains('Narration', na=False)]\n",
    "    \n",
    "    # Debit and Credit amounts\n",
    "    sample_data['Debit Amount'] = sample_data['Debit Amount'].abs()\n",
    "    sample_data['Credit Amount'] = -sample_data['Credit Amount'].abs()\n",
    "    \n",
    "    # V-Lookup from Master file\n",
    "    sample_data = pd.merge(sample_data, master_data[['Particulars', 'Header']], on='Particulars', how='left')\n",
    "    \n",
    "    # first output (Processed Data)\n",
    "    sample_data.to_csv(output_path_1, index=False)\n",
    "    \n",
    "    # data for Output 2 using pivot_table to handle duplicates\n",
    "    reshaped_data = sample_data.pivot_table(\n",
    "        index=['Date', 'Vch Type', 'Vch No.'],\n",
    "        columns='Particulars',\n",
    "        values='Amount',\n",
    "        aggfunc='sum'  # You can change this to other functions depending on your needs\n",
    "    ).reset_index()\n",
    "    \n",
    "    # Concatenate fixed particular\n",
    "    fixed_particular = sample_data['Particulars'].iloc[0]\n",
    "    reshaped_data.insert(0, 'Fixed Particular', fixed_particular)\n",
    "    \n",
    "    # second output (Reshaped Data)\n",
    "    reshaped_data.to_csv(output_path_2, index=False)\n",
    "    print(f\"Processed data saved to {output_path_1} and reshaped data saved to {output_path_2}\")\n",
    "\n",
    "# Example usage\n",
    "sample_data_path = 'Sample Data.csv'  # Replace with actual file path\n",
    "master_data_path = 'Master.csv'       # Replace with actual file path\n",
    "output_path_1 = 'Processed_Data.csv'\n",
    "output_path_2 = 'Reshaped_Data.csv'\n",
    "\n",
    "process_tax_audit_data(sample_data_path, master_data_path, output_path_1, output_path_2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ab497ad",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
